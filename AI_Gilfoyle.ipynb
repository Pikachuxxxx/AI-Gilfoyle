{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Gilfoyle.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4rAIIX3lZOwh"
      ],
      "authorship_tag": "ABX9TyPBhmrtQzuO+pMXwwHSW4fW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pikachuxxxx/AI-Gilfoyle/blob/master/AI_Gilfoyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBL4g7kUvdu",
        "colab_type": "text"
      },
      "source": [
        "## Let's mark the beginning of a hero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VBP9TZ7WDuB",
        "colab_type": "text"
      },
      "source": [
        "#### Uploading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxLSehUdWEfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtF-D2PyVzrs",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2v_QiCATuWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "06a285aa-7ee7-4a35-9eaf-93f3f7a4c54f"
      },
      "source": [
        "# Installing the dependencies\n",
        "!pip install tensorflow==1.15\n",
        "!pip install tflearn \n",
        "\n",
        "# things we need for NLP\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# things we need for Tensorflow\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |███████████████████████████████▋| 407.1MB 1.2MB/s eta 0:00:05"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zKhYXtRV71j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import our chat-bot intents file\n",
        "import json\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rAIIX3lZOwh",
        "colab_type": "text"
      },
      "source": [
        "#### Organising dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuoQxDXVZL3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}